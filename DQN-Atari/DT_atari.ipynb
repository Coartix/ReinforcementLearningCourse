{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: gym 0.26.2\n",
      "Uninstalling gym-0.26.2:\n",
      "  Successfully uninstalled gym-0.26.2\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gym\n",
      "  Using cached gym-0.26.2-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/coartix/.local/lib/python3.10/site-packages (from gym) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/coartix/.local/lib/python3.10/site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/coartix/.local/lib/python3.10/site-packages (from gym) (0.0.8)\n",
      "Installing collected packages: gym\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dopamine-rl 4.0.6 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed gym-0.26.2\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gym[atari] in /home/coartix/.local/lib/python3.10/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/coartix/.local/lib/python3.10/site-packages (from gym[atari]) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/coartix/.local/lib/python3.10/site-packages (from gym[atari]) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/coartix/.local/lib/python3.10/site-packages (from gym[atari]) (0.0.8)\n",
      "Requirement already satisfied: ale-py~=0.8.0 in /home/coartix/.local/lib/python3.10/site-packages (from gym[atari]) (0.8.1)\n",
      "Requirement already satisfied: importlib-resources in /home/coartix/.local/lib/python3.10/site-packages (from ale-py~=0.8.0->gym[atari]) (6.1.0)\n",
      "Requirement already satisfied: typing-extensions in /home/coartix/.local/lib/python3.10/site-packages (from ale-py~=0.8.0->gym[atari]) (4.7.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gym[accept-rom-license] in /home/coartix/.local/lib/python3.10/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/coartix/.local/lib/python3.10/site-packages (from gym[accept-rom-license]) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/coartix/.local/lib/python3.10/site-packages (from gym[accept-rom-license]) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/coartix/.local/lib/python3.10/site-packages (from gym[accept-rom-license]) (0.0.8)\n",
      "Requirement already satisfied: autorom~=0.4.2 in /home/coartix/.local/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (0.4.2)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (8.0.3)\n",
      "Requirement already satisfied: requests in /home/coartix/.local/lib/python3.10/site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (2.28.2)\n",
      "Requirement already satisfied: tqdm in /home/coartix/.local/lib/python3.10/site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (4.65.0)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in /home/coartix/.local/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/coartix/.local/lib/python3.10/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/coartix/.local/lib/python3.10/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/coartix/.local/lib/python3.10/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install gym\n",
    "!pip install gym[atari]\n",
    "!pip install gym[accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Implementation of a Decision Transformer for the Atari game Breakout\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from preprocess import CustomWrapper\n",
    "from ReplayBuffer import ReplayBuffer\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "env = gym.make('BreakoutNoFrameskip-v4', render_mode='rgb_array')\n",
    "# Preprocess the environment (important if you want luminance to put grayscale as False)\n",
    "env = CustomWrapper(env, noop_max=30, frame_skip=4, screen_size=84, luminance_obs=True, grayscale_obs=False)\n",
    "# Stack 4 frames\n",
    "env = gym.wrappers.FrameStack(env, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  (4, 84, 84)\n",
      "Image type:  uint8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f00737ff1c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "img, _, _, _, _ = env.step(0)\n",
    "print('Image shape: ', img.shape)\n",
    "print('Image type: ', img.dtype)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/coartix/S9-EPITA/Reinforcement Learning/DQN-Atari/DT_atari.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/coartix/S9-EPITA/Reinforcement%20Learning/DQN-Atari/DT_atari.ipynb#W2sZmlsZQ%3D%3D?line=228'>229</a>\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/coartix/S9-EPITA/Reinforcement%20Learning/DQN-Atari/DT_atari.ipynb#W2sZmlsZQ%3D%3D?line=229'>230</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/coartix/S9-EPITA/Reinforcement%20Learning/DQN-Atari/DT_atari.ipynb#W2sZmlsZQ%3D%3D?line=230'>231</a>\u001b[0m     \u001b[39m# Get action\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/coartix/S9-EPITA/Reinforcement%20Learning/DQN-Atari/DT_atari.ipynb#W2sZmlsZQ%3D%3D?line=231'>232</a>\u001b[0m     action, log_probs \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mget_action_and_log_probs(obs, agent_state, pad_mask\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/coartix/S9-EPITA/Reinforcement%20Learning/DQN-Atari/DT_atari.ipynb#W2sZmlsZQ%3D%3D?line=232'>233</a>\u001b[0m     \u001b[39m# Take step\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/coartix/S9-EPITA/Reinforcement%20Learning/DQN-Atari/DT_atari.ipynb#W2sZmlsZQ%3D%3D?line=233'>234</a>\u001b[0m     next_obs, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "\u001b[1;32m/home/coartix/S9-EPITA/Reinforcement Learning/DQN-Atari/DT_atari.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/coartix/S9-EPITA/Reinforcement%20Learning/DQN-Atari/DT_atari.ipynb#W2sZmlsZQ%3D%3D?line=179'>180</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_action_and_log_probs\u001b[39m(\u001b[39mself\u001b[39m, obs, agent_state, pad_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/coartix/S9-EPITA/Reinforcement%20Learning/DQN-Atari/DT_atari.ipynb#W2sZmlsZQ%3D%3D?line=180'>181</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/coartix/S9-EPITA/Reinforcement%20Learning/DQN-Atari/DT_atari.ipynb#W2sZmlsZQ%3D%3D?line=181'>182</a>\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(obs, agent_state, pad_mask, return_logits\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/coartix/S9-EPITA/Reinforcement%20Learning/DQN-Atari/DT_atari.ipynb#W2sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m     probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(model_outputs[\u001b[39m'\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m'\u001b[39m], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/coartix/S9-EPITA/Reinforcement%20Learning/DQN-Atari/DT_atari.ipynb#W2sZmlsZQ%3D%3D?line=183'>184</a>\u001b[0m     action \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmultinomial(probs, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/home/coartix/S9-EPITA/Reinforcement Learning/DQN-Atari/DT_atari.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/coartix/S9-EPITA/Reinforcement%20Learning/DQN-Atari/DT_atari.ipynb#W2sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, obs, agent_state, pad_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, return_logits\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/coartix/S9-EPITA/Reinforcement%20Learning/DQN-Atari/DT_atari.ipynb#W2sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m     \u001b[39m# obs: [batch_size, seq_len, obs_shape]\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/coartix/S9-EPITA/Reinforcement%20Learning/DQN-Atari/DT_atari.ipynb#W2sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m     \u001b[39m# agent_state: [batch_size, d_model]\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/coartix/S9-EPITA/Reinforcement%20Learning/DQN-Atari/DT_atari.ipynb#W2sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m     batch_size \u001b[39m=\u001b[39m obs\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/coartix/S9-EPITA/Reinforcement%20Learning/DQN-Atari/DT_atari.ipynb#W2sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m     seq_len \u001b[39m=\u001b[39m obs\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/coartix/S9-EPITA/Reinforcement%20Learning/DQN-Atari/DT_atari.ipynb#W2sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m     \u001b[39m# Get actions\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "def get_action_space():\n",
    "    return env.action_space.n\n",
    "\n",
    "def get_obs_space():\n",
    "    return env.observation_space.shape\n",
    "\n",
    "class DT(nn.Module):\n",
    "    def __init__(self, obs_shape, action_shape, num_layers, d_model, num_heads, dff, max_seq_len, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.obs_shape = obs_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.obs_embedding = nn.Linear(obs_shape[0], d_model)\n",
    "        self.action_embedding = nn.Embedding(action_shape, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, dff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, action_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, actions, pad_mask=None, return_logits=True):\n",
    "        batch_size = obs.shape[0]\n",
    "        seq_len = obs.shape[1]\n",
    "\n",
    "        obs = self.obs_embedding(obs)\n",
    "        actions = self.action_embedding(actions)\n",
    "        pos = torch.arange(seq_len, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        pos = self.pos_embedding(pos)\n",
    "\n",
    "        x = obs + actions + pos\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, pad_mask)\n",
    "\n",
    "        if return_logits:\n",
    "            x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.mha = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, dff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dff, d_model)\n",
    "        )\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, pad_mask=None):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        # pad_mask: [batch_size, seq_len]\n",
    "\n",
    "        # Multihead Attention\n",
    "        attn_out, _ = self.mha(x, x, x, key_padding_mask=pad_mask)\n",
    "        attn_out = self.dropout1(attn_out)\n",
    "        out1 = self.ln1(x + attn_out)\n",
    "\n",
    "        # MLP\n",
    "        mlp_out = self.mlp(out1)\n",
    "        mlp_out = self.dropout2(mlp_out)\n",
    "        out2 = self.ln2(out1 + mlp_out)\n",
    "\n",
    "        return out2\n",
    "    \n",
    "class DTAgent(nn.Module):\n",
    "    def __init__(self, obs_shape, action_shape, num_layers, d_model, num_heads, dff, max_seq_len, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.dt = DT(obs_shape, action_shape, num_layers, d_model, num_heads, dff, max_seq_len, dropout=dropout)\n",
    "        self.obs_shape = obs_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, obs, agent_state, pad_mask=None, return_logits=True):\n",
    "        # obs: [batch_size, seq_len, obs_shape]\n",
    "        # agent_state: [batch_size, d_model]\n",
    "        batch_size = obs.shape[0]\n",
    "        seq_len = obs.shape[1]\n",
    "\n",
    "        # Get actions\n",
    "        actions = torch.randint(self.action_shape, (batch_size, seq_len), device=device)\n",
    "        # Get DT logits\n",
    "        logits = self.dt(obs, actions, pad_mask, return_logits)\n",
    "        # Get action probabilities\n",
    "        probs = nn.functional.softmax(logits, dim=-1)\n",
    "        # Sample actions\n",
    "        actions = torch.multinomial(probs, 1).squeeze(-1)\n",
    "        # Get log probs\n",
    "        log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "        log_probs = torch.gather(log_probs, dim=-1, index=actions.unsqueeze(-1)).squeeze(-1)\n",
    "        # Get entropy\n",
    "        entropy = -(log_probs * probs).sum(-1)\n",
    "\n",
    "        agent_state = torch.zeros(batch_size, self.d_model, device=device)\n",
    "\n",
    "        model_outputs = {\n",
    "            'action': actions,\n",
    "            'log_probs': log_probs,\n",
    "            'entropy': entropy,\n",
    "            'agent_state': agent_state\n",
    "        }\n",
    "\n",
    "        return model_outputs\n",
    "    \n",
    "    def initial_state(self, batch_size, device):\n",
    "        return torch.zeros(batch_size, self.d_model, device=device)\n",
    "    \n",
    "    def get_action(self, obs, agent_state, pad_mask=None, return_logits=False):\n",
    "        with torch.no_grad():\n",
    "            model_outputs = self.forward(obs, agent_state, pad_mask, return_logits)\n",
    "        return model_outputs\n",
    "    \n",
    "    def evaluate_actions(self, obs, agent_state, actions, pad_mask=None):\n",
    "        model_outputs = self.forward(obs, agent_state, pad_mask, return_logits=True)\n",
    "        log_probs = model_outputs['log_probs']\n",
    "        entropy = model_outputs['entropy']\n",
    "        return log_probs, entropy\n",
    "    \n",
    "    def get_value(self, obs, agent_state, pad_mask=None):\n",
    "        with torch.no_grad():\n",
    "            model_outputs = self.forward(obs, agent_state, pad_mask, return_logits=True)\n",
    "        logits = model_outputs['logits']\n",
    "        return logits\n",
    "    \n",
    "    def get_probs(self, obs, agent_state, pad_mask=None):\n",
    "        with torch.no_grad():\n",
    "            model_outputs = self.forward(obs, agent_state, pad_mask, return_logits=True)\n",
    "        probs = nn.functional.softmax(model_outputs['logits'], dim=-1)\n",
    "        return probs\n",
    "    \n",
    "    def get_log_probs(self, obs, agent_state, pad_mask=None):\n",
    "        with torch.no_grad():\n",
    "            model_outputs = self.forward(obs, agent_state, pad_mask, return_logits=True)\n",
    "        log_probs = nn.functional.log_softmax(model_outputs['logits'], dim=-1)\n",
    "        return log_probs\n",
    "    \n",
    "    def get_entropy(self, obs, agent_state, pad_mask=None):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_outputs = self.forward(obs, agent_state, pad_mask, return_logits=True)\n",
    "        probs = nn.functional.softmax(model_outputs['logits'], dim=-1)\n",
    "        entropy = -(probs * model_outputs['log_probs']).sum(-1)\n",
    "        return entropy\n",
    "    \n",
    "    def get_action_and_value(self, obs, agent_state, pad_mask=None):\n",
    "        with torch.no_grad():\n",
    "            model_outputs = self.forward(obs, agent_state, pad_mask, return_logits=True)\n",
    "        probs = nn.functional.softmax(model_outputs['logits'], dim=-1)\n",
    "        action = torch.multinomial(probs, 1).squeeze(-1)\n",
    "        return action, model_outputs['logits']\n",
    "    \n",
    "    def get_action_and_log_probs(self, obs, agent_state, pad_mask=None):\n",
    "        with torch.no_grad():\n",
    "            model_outputs = self.forward(obs, agent_state, pad_mask, return_logits=True)\n",
    "        probs = nn.functional.softmax(model_outputs['logits'], dim=-1)\n",
    "        action = torch.multinomial(probs, 1).squeeze(-1)\n",
    "        log_probs = nn.functional.log_softmax(model_outputs['logits'], dim=-1)\n",
    "        log_probs = torch.gather(log_probs, dim=-1, index=action.unsqueeze(-1)).squeeze(-1)\n",
    "        return action, log_probs\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "obs_shape = get_obs_space()\n",
    "action_shape = get_action_space()\n",
    "num_layers = 2\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "dff = 1024\n",
    "max_seq_len = 4\n",
    "\n",
    "# Initialize the agent\n",
    "agent = DTAgent(obs_shape, action_shape, num_layers, d_model, num_heads, dff, max_seq_len, dropout=0.0).to(device)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize the replay buffer\n",
    "replay_buffer = ReplayBuffer(100000)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the agent state\n",
    "agent_state = agent.initial_state(batch_size=1, device=device)\n",
    "# Initialize the target agent state\n",
    "agent_target_state = agent.initial_state(batch_size=1, device=device)\n",
    "\n",
    "# Initialize the total reward\n",
    "total_reward = 0\n",
    "# Initialize the total steps\n",
    "total_steps = 0\n",
    "# Initialize the episode length\n",
    "episode_length = 0\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for i in range(100000):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Get action\n",
    "        action, log_probs = agent.get_action_and_log_probs(obs, agent_state, pad_mask=None)\n",
    "        # Take step\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        # Add to replay buffer\n",
    "        replay_buffer.add(obs, action, reward, next_obs, done)\n",
    "        # Update obs\n",
    "        obs = next_obs\n",
    "\n",
    "        # Sample from replay buffer\n",
    "        obs, action, reward, next_obs, done = replay_buffer.sample(32)\n",
    "\n",
    "        # Compute loss\n",
    "        logits = agent.get_value(obs, agent_state, pad_mask=None)\n",
    "        loss = loss_fn(logits, reward)\n",
    "\n",
    "        # Backpropagate\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update agent state\n",
    "        agent_state = agent_target_state\n",
    "        # Update total reward\n",
    "        total_reward += reward\n",
    "        # Update total steps\n",
    "        total_steps += 1\n",
    "        # Update episode length\n",
    "        episode_length += 1\n",
    "\n",
    "        # Update target agent state\n",
    "        agent_target_state = agent.initial_state(batch_size=1, device=device)\n",
    "\n",
    "        # Update target network\n",
    "        if total_steps % 1000 == 0:\n",
    "            agent_target_state.load_state_dict(agent_state.state_dict())\n",
    "\n",
    "        # Print info\n",
    "        if total_steps % 1000 == 0:\n",
    "            print(f\"Total reward: {total_reward}, Episode length: {episode_length}\")\n",
    "            total_reward = 0\n",
    "            episode_length = 0\n",
    "\n",
    "# Save the model\n",
    "torch.save(agent.state_dict(), 'models/dt_atari.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
